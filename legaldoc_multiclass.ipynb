{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#mount the drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Verify the contents of the directory\n",
        "!ls /content/drive/MyDrive/legaldoc_multilabel/CSVs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZjgdc07T0An",
        "outputId": "bf85e0b1-8869-429c-9bb7-ee369f10c74f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "test_data  test_outputs  test_summaries  train_data  val_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "C-WkoC8V3UG5",
        "outputId": "8681ddb9-0cf9-4aa7-99af-60e4dac64944"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      Sentence,Label\n",
              "0  for the sake of convenience we refer to the fa...\n",
              "1  this matter is a classic illustration of the c...\n",
              "2  quite often the commencement of an act is post...\n",
              "3  provision is also at times made for appointmen...\n",
              "4  this is what has exactly happened in this case..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1930b5f2-e836-49f2-b6bc-b9898966a51f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence,Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>for the sake of convenience we refer to the fa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>this matter is a classic illustration of the c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>quite often the commencement of an act is post...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>provision is also at times made for appointmen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>this is what has exactly happened in this case...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1930b5f2-e836-49f2-b6bc-b9898966a51f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1930b5f2-e836-49f2-b6bc-b9898966a51f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1930b5f2-e836-49f2-b6bc-b9898966a51f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fec60f8d-7be3-4667-b879-dd4bc1dcd324\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fec60f8d-7be3-4667-b879-dd4bc1dcd324')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fec60f8d-7be3-4667-b879-dd4bc1dcd324 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_val_data",
              "summary": "{\n  \"name\": \"train_val_data\",\n  \"rows\": 8656,\n  \"fields\": [\n    {\n      \"column\": \"Sentence,Label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8596,\n        \"samples\": [\n          \"the ritz hotel v charles of the ritz 1989,none\",\n          \"a that abdul khader because he was a tenant between january1942 to january1948 for six years therefore was a protected tenant under sub cl ii of cl 1 of s 34 of the andhra pradesh act b that abdul khader held the land from october1943 to october1949therefore was a protected tenant of sail gulshan under sub cl iii of cl 1 of s 34 of act 21 of 1950 in these circumstances the high court held that adbul khader was entitled to 60 of the compensation paid,none\",\n          \"thus the determining factor is the real nature of principal order passed by the single judge which is appealed against and neither the mentioning in the cause title of the application of both the articles nor the granting of ancillary orders thereupon made by learned single judge would be relevant,none\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the paths to the data folders\n",
        "train_data_path = '/content/drive/MyDrive/legaldoc_multilabel/CSVs/train_data'\n",
        "val_data_path = '/content/drive/MyDrive/legaldoc_multilabel/CSVs/val_data'\n",
        "test_data_path = '/content/drive/MyDrive/legaldoc_multilabel/CSVs/test_data'\n",
        "\n",
        "\n",
        "def load_data_from_folder(folder_path):\n",
        "    data_frames = []\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith('.csv'):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            df = pd.read_csv(file_path, delimiter='\\t')\n",
        "            data_frames.append(df)\n",
        "    return pd.concat(data_frames, ignore_index=True)\n",
        "\n",
        "# Load data from folders\n",
        "train_data = load_data_from_folder(train_data_path)\n",
        "val_data = load_data_from_folder(val_data_path)\n",
        "test_data = load_data_from_folder(test_data_path)\n",
        "\n",
        "# Combine train and validation data for training\n",
        "train_val_data = pd.concat([train_data, val_data], ignore_index=True)\n",
        "\n",
        "# Check the first few rows of the combined data\n",
        "train_val_data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_val_data.columns)\n",
        "print(test_data.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBqsmA9oWXCp",
        "outputId": "48a79c5b-e646-4f3a-8e8a-4c160b0d2493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Sentence,Label'], dtype='object')\n",
            "Index(['Sentence,Label'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[torch] accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoRjiW25W9kZ",
        "outputId": "bcb2e093-ba7b-4ebb-8c68-d980f2538737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.0+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.30.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset\n",
        "# from transformers import BertTokenizer, BertForSequenceClassification\n",
        "# from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# # Read the data (assuming train_val_data and test_data are already loaded)\n",
        "\n",
        "# # Split the single column into two columns\n",
        "# train_val_data[['Sentence', 'Label']] = train_val_data['Sentence,Label'].str.split(',', expand=True)\n",
        "# test_data[['Sentence', 'Label']] = test_data['Sentence,Label'].str.split(',', expand=True)\n",
        "\n",
        "# # Drop the original combined column\n",
        "# train_val_data = train_val_data.drop(columns=['Sentence,Label'])\n",
        "# test_data = test_data.drop(columns=['Sentence,Label'])\n",
        "\n",
        "# # Initialize the tokenizer\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# # Custom Dataset class\n",
        "# class CustomDataset(Dataset):\n",
        "#     def __init__(self, sentences, labels, tokenizer):\n",
        "#         self.encodings = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "#         self.labels = torch.tensor(labels)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "#         item['labels'] = self.labels[idx]\n",
        "#         return item\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.labels)\n",
        "\n",
        "# # Prepare the data for BERT\n",
        "# train_sentences = train_val_data['Sentence'].tolist()\n",
        "# train_labels = train_val_data['Label'].astype('category').cat.codes.tolist()\n",
        "# test_sentences = test_data['Sentence'].tolist()\n",
        "# test_labels = test_data['Label'].astype('category').cat.codes.tolist()\n",
        "\n",
        "# train_dataset = CustomDataset(train_sentences, train_labels, tokenizer)\n",
        "# test_dataset = CustomDataset(test_sentences, test_labels, tokenizer)\n",
        "\n",
        "# # Initialize the BERT model\n",
        "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
        "\n",
        "# # Set up training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir='./results',\n",
        "#     num_train_epochs=3,\n",
        "#     per_device_train_batch_size=8,\n",
        "#     per_device_eval_batch_size=8,\n",
        "#     warmup_steps=500,\n",
        "#     weight_decay=0.01,\n",
        "#     logging_dir='./logs',\n",
        "# )\n",
        "\n",
        "# # Define the Trainer\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=train_dataset,\n",
        "#     eval_dataset=test_dataset,\n",
        "# )\n",
        "\n",
        "# # Train the model\n",
        "# trainer.train()\n"
      ],
      "metadata": {
        "id": "UJereAwOUVw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train inputs keys:\", train_inputs.keys())\n",
        "print(\"Test inputs keys:\", test_inputs.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S58Csxed2YDG",
        "outputId": "d19d9069-5017-4cf1-e68b-6af65b9f0ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train inputs keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
            "Test inputs keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "# from transformers import Trainer, TrainingArguments\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import pandas as pd\n",
        "\n",
        "# # Define a custom dataset class\n",
        "# class CustomDataset(Dataset):\n",
        "#     def __init__(self, inputs):\n",
        "#         self.inputs = inputs\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.inputs['input_ids'])\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         return {key: self.inputs[key][idx] for key in self.inputs.keys()}\n",
        "\n",
        "# # Initialize the tokenizer\n",
        "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# # Tokenize the data\n",
        "# def tokenize_data(sentences, labels, tokenizer):\n",
        "#     inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "#     inputs['labels'] = torch.tensor(labels)\n",
        "#     return inputs\n",
        "\n",
        "# # Prepare the data for DistilBERT\n",
        "# train_sentences = train_val_data['Sentence'].tolist()\n",
        "# train_labels = train_val_data['Label'].astype('category').cat.codes.tolist()\n",
        "# test_sentences = test_data['Sentence'].tolist()\n",
        "# test_labels = test_data['Label'].astype('category').cat.codes.tolist()\n",
        "\n",
        "# train_inputs = tokenize_data(train_sentences, train_labels, tokenizer)\n",
        "# test_inputs = tokenize_data(test_sentences, test_labels, tokenizer)\n",
        "\n",
        "# # Convert inputs to custom dataset\n",
        "# train_dataset = CustomDataset(train_inputs)\n",
        "# test_dataset = CustomDataset(test_inputs)\n",
        "\n",
        "# # Initialize the DistilBERT model\n",
        "# model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5)\n",
        "\n",
        "# # Set up training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir='./results',\n",
        "#     num_train_epochs=3,\n",
        "#     per_device_train_batch_size=8,\n",
        "#     per_device_eval_batch_size=8,\n",
        "#     warmup_steps=500,\n",
        "#     weight_decay=0.01,\n",
        "#     logging_dir='./logs',\n",
        "# )\n",
        "\n",
        "# # Define the Trainer\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=train_dataset,\n",
        "#     eval_dataset=test_dataset,\n",
        "# )\n",
        "\n",
        "# # Train the model\n",
        "# trainer.train()\n"
      ],
      "metadata": {
        "id": "OLrRTM6M1R_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "# Define the paths to the data folders\n",
        "train_data_path = '/content/drive/MyDrive/legaldoc_multilabel/CSVs/train_data'\n",
        "val_data_path = '/content/drive/MyDrive/legaldoc_multilabel/CSVs/val_data'\n",
        "test_data_path = '/content/drive/MyDrive/legaldoc_multilabel/CSVs/test_data'\n",
        "\n",
        "def load_data_from_folder(folder_path):\n",
        "    data_frames = []\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith('.csv'):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            df = pd.read_csv(file_path)\n",
        "            data_frames.append(df)\n",
        "    return pd.concat(data_frames, ignore_index=True)\n",
        "\n",
        "# Load data from folders\n",
        "train_data = load_data_from_folder(train_data_path)\n",
        "val_data = load_data_from_folder(val_data_path)\n",
        "test_data = load_data_from_folder(test_data_path)\n",
        "\n",
        "# Combine train and validation data for training\n",
        "train_val_data = pd.concat([train_data, val_data], ignore_index=True)\n",
        "\n",
        "# Check columns in the loaded data frames\n",
        "print(train_data.columns)\n",
        "print(val_data.columns)\n",
        "print(test_data.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhqebgjL9csL",
        "outputId": "db976674-4739-4b28-dab0-73cb8bcc5f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Sentence', 'Label'], dtype='object')\n",
            "Index(['Sentence', 'Label'], dtype='object')\n",
            "Index(['Sentence', 'Label'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "# Define the paths to the data folders\n",
        "train_data_path = '/content/drive/MyDrive/legaldoc_multilabel/CSVs/train_data'\n",
        "val_data_path = '/content/drive/MyDrive/legaldoc_multilabel/CSVs/val_data'\n",
        "test_data_path = '/content/drive/MyDrive/legaldoc_multilabel/CSVs/test_data'\n",
        "\n",
        "def load_data_from_folder(folder_path):\n",
        "    data_frames = []\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith('.csv'):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            df = pd.read_csv(file_path)\n",
        "            data_frames.append(df)\n",
        "    return pd.concat(data_frames, ignore_index=True)\n",
        "\n",
        "# Load data from folders\n",
        "train_data = load_data_from_folder(train_data_path)\n",
        "val_data = load_data_from_folder(val_data_path)\n",
        "test_data = load_data_from_folder(test_data_path)\n",
        "\n",
        "# Combine train and validation data for training\n",
        "train_val_data = pd.concat([train_data, val_data], ignore_index=True)\n",
        "\n",
        "# Preprocessing\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_val_data['Sentence'])\n",
        "X_train_val = tokenizer.texts_to_sequences(train_val_data['Sentence'])\n",
        "X_test = tokenizer.texts_to_sequences(test_data['Sentence'])\n",
        "\n",
        "maxlen = 100  # Maximum sequence length\n",
        "X_train_val = pad_sequences(X_train_val, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(train_val_data['Label'])\n",
        "y_train_val = label_encoder.transform(train_val_data['Label'])\n",
        "y_test = label_encoder.transform(test_data['Label'])\n",
        "\n",
        "# Split train_val_data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))  # 5 output classes for multiclass classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-QUtXys30QB",
        "outputId": "a6cddbcc-3ba1-4bfd-be31-0253d2055b19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "217/217 [==============================] - 6s 23ms/step - loss: 0.7837 - accuracy: 0.7750 - val_loss: 0.7332 - val_accuracy: 0.7789\n",
            "Epoch 2/10\n",
            "217/217 [==============================] - 4s 19ms/step - loss: 0.5679 - accuracy: 0.7956 - val_loss: 0.6854 - val_accuracy: 0.7858\n",
            "Epoch 3/10\n",
            "217/217 [==============================] - 3s 16ms/step - loss: 0.2317 - accuracy: 0.9263 - val_loss: 0.7785 - val_accuracy: 0.7627\n",
            "Epoch 4/10\n",
            "217/217 [==============================] - 6s 25ms/step - loss: 0.0661 - accuracy: 0.9837 - val_loss: 0.8810 - val_accuracy: 0.7506\n",
            "Epoch 5/10\n",
            "217/217 [==============================] - 4s 18ms/step - loss: 0.0248 - accuracy: 0.9941 - val_loss: 1.0202 - val_accuracy: 0.7598\n",
            "Epoch 6/10\n",
            "217/217 [==============================] - 3s 15ms/step - loss: 0.0133 - accuracy: 0.9965 - val_loss: 1.2140 - val_accuracy: 0.7742\n",
            "Epoch 7/10\n",
            "217/217 [==============================] - 4s 17ms/step - loss: 0.0091 - accuracy: 0.9974 - val_loss: 1.1749 - val_accuracy: 0.7610\n",
            "Epoch 8/10\n",
            "217/217 [==============================] - 6s 29ms/step - loss: 0.0078 - accuracy: 0.9974 - val_loss: 1.2632 - val_accuracy: 0.7696\n",
            "Epoch 9/10\n",
            "217/217 [==============================] - 5s 25ms/step - loss: 0.0070 - accuracy: 0.9971 - val_loss: 1.3073 - val_accuracy: 0.7650\n",
            "Epoch 10/10\n",
            "217/217 [==============================] - 4s 20ms/step - loss: 0.0057 - accuracy: 0.9975 - val_loss: 1.3543 - val_accuracy: 0.7644\n",
            "58/58 [==============================] - 1s 7ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    analysis       0.96      0.81      0.88       221\n",
            "    argument       1.00      0.94      0.97        32\n",
            "       facts       0.96      0.85      0.90        92\n",
            "   judgement       1.00      0.59      0.74        27\n",
            "        none       0.96      0.99      0.98      1455\n",
            "\n",
            "    accuracy                           0.96      1827\n",
            "   macro avg       0.98      0.84      0.89      1827\n",
            "weighted avg       0.96      0.96      0.96      1827\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the paths to the data folders\n",
        "train_data_path = '/content/drive/MyDrive/legaldoc_multilabel/CSVs/train_data'\n",
        "val_data_path = '/content/drive/MyDrive/legaldoc_multilabel/CSVs/val_data'\n",
        "test_data_path = '/content/drive/MyDrive/legaldoc_multilabel/CSVs/test_data'\n",
        "\n",
        "def load_data_from_folder(folder_path):\n",
        "    data_frames = []\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith('.csv'):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            df = pd.read_csv(file_path)\n",
        "            data_frames.append(df)\n",
        "    return pd.concat(data_frames, ignore_index=True)\n",
        "\n",
        "# Load data from folders\n",
        "train_data = load_data_from_folder(train_data_path)\n",
        "val_data = load_data_from_folder(val_data_path)\n",
        "test_data = load_data_from_folder(test_data_path)\n",
        "\n",
        "# Combine train and validation data for training\n",
        "train_val_data = pd.concat([train_data, val_data], ignore_index=True)\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_val = vectorizer.fit_transform(train_val_data['Sentence'])\n",
        "X_test = vectorizer.transform(test_data['Sentence'])\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(train_val_data['Label'])\n",
        "y_train_val = label_encoder.transform(train_val_data['Label'])\n",
        "y_test = label_encoder.transform(test_data['Label'])\n",
        "\n",
        "# Split train_val_data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "id": "UPq0jKs1ArQ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "146807f4-6eca-4337-f22d-93e89e1b2a0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    analysis       0.95      0.08      0.15       221\n",
            "    argument       0.00      0.00      0.00        32\n",
            "       facts       0.80      0.04      0.08        92\n",
            "   judgement       0.00      0.00      0.00        27\n",
            "        none       0.81      1.00      0.89      1455\n",
            "\n",
            "    accuracy                           0.81      1827\n",
            "   macro avg       0.51      0.22      0.22      1827\n",
            "weighted avg       0.80      0.81      0.73      1827\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Define the paths to the data folders\n",
        "train_data_path = '/content/drive/MyDrive/legaldoc_multilabel/CSVs/train_data'\n",
        "val_data_path = '/content/drive/MyDrive/legaldoc_multilabel/CSVs/val_data'\n",
        "test_data_path = '/content/drive/MyDrive/legaldoc_multilabel/CSVs/test_data'\n",
        "\n",
        "def load_data_from_folder(folder_path):\n",
        "    data_frames = []\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith('.csv'):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            df = pd.read_csv(file_path)\n",
        "            data_frames.append(df)\n",
        "    return pd.concat(data_frames, ignore_index=True)\n",
        "\n",
        "# Load data from folders\n",
        "train_data = load_data_from_folder(train_data_path)\n",
        "val_data = load_data_from_folder(val_data_path)\n",
        "test_data = load_data_from_folder(test_data_path)\n",
        "\n",
        "# Combine train and validation data for training\n",
        "train_val_data = pd.concat([train_data, val_data], ignore_index=True)\n",
        "\n",
        "# Preprocess the text data\n",
        "max_num_words = 10000\n",
        "max_sequence_length = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_num_words)\n",
        "tokenizer.fit_on_texts(train_val_data['Sentence'])\n",
        "X_train_val = tokenizer.texts_to_sequences(train_val_data['Sentence'])\n",
        "X_test = tokenizer.texts_to_sequences(test_data['Sentence'])\n",
        "\n",
        "X_train_val = pad_sequences(X_train_val, maxlen=max_sequence_length)\n",
        "X_test = pad_sequences(X_test, maxlen=max_sequence_length)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(train_val_data['Label'])\n",
        "y_train_val = to_categorical(label_encoder.transform(train_val_data['Label']))\n",
        "y_test = to_categorical(label_encoder.transform(test_data['Label']))\n",
        "\n",
        "# Split train_val_data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_num_words, output_dim=128, input_length=max_sequence_length))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = y_pred.argmax(axis=1)\n",
        "y_test_classes = y_test.argmax(axis=1)\n",
        "print(classification_report(y_test_classes, y_pred_classes, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97AUYoWZ0R50",
        "outputId": "9feb5042-7262-4cdb-bbe4-cb6d74f02e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "109/109 [==============================] - 57s 409ms/step - loss: 0.8315 - accuracy: 0.7699 - val_loss: 0.7435 - val_accuracy: 0.7789\n",
            "Epoch 2/5\n",
            "109/109 [==============================] - 41s 377ms/step - loss: 0.6872 - accuracy: 0.7787 - val_loss: 0.7352 - val_accuracy: 0.7771\n",
            "Epoch 3/5\n",
            "109/109 [==============================] - 41s 374ms/step - loss: 0.5088 - accuracy: 0.8182 - val_loss: 0.7502 - val_accuracy: 0.7610\n",
            "Epoch 4/5\n",
            "109/109 [==============================] - 39s 362ms/step - loss: 0.3505 - accuracy: 0.8794 - val_loss: 0.9146 - val_accuracy: 0.7182\n",
            "Epoch 5/5\n",
            "109/109 [==============================] - 39s 356ms/step - loss: 0.2408 - accuracy: 0.9167 - val_loss: 1.0422 - val_accuracy: 0.6773\n",
            "58/58 [==============================] - 2s 27ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    analysis       0.75      0.78      0.76       221\n",
            "    argument       0.73      0.59      0.66        32\n",
            "       facts       0.73      0.82      0.77        92\n",
            "   judgement       0.50      0.19      0.27        27\n",
            "        none       0.95      0.95      0.95      1455\n",
            "\n",
            "    accuracy                           0.90      1827\n",
            "   macro avg       0.73      0.66      0.68      1827\n",
            "weighted avg       0.90      0.90      0.90      1827\n",
            "\n"
          ]
        }
      ]
    }
  ]
}